{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import ResNet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, utils, datasets\n",
    "from argparse import ArgumentParser\n",
    "from torchvision import transforms as tt\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# set manual seed for reproducibility\n",
    "# seed = 42\n",
    "\n",
    "# # general reproducibility\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "# gpu training specific\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as T\n",
    "import glob, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file, header=None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract 11 input values and 1 target value\n",
    "        inputs = self.data.iloc[idx, :-1].values.astype(np.float32)\n",
    "        target = self.data.iloc[idx, -1].astype(np.float32)\n",
    "\n",
    "        sample = {'inputs': inputs, 'target': target}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientUpdate(object):\n",
    "    def __init__(self, dataset, batchSize, learning_rate, epochs, idxs, sch_flag):\n",
    "        self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batchSize, shuffle=True)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.sch_flag = sch_flag\n",
    "\n",
    "    def train(self, model):\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        # optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.95, weight_decay = 5e-4)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "        # if self.sch_flag == True:\n",
    "        #    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5)\n",
    "        # my_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "        e_loss = []\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "\n",
    "            train_loss = 0.0\n",
    "\n",
    "            model.train()\n",
    "            for data, labels in self.train_loader:\n",
    "                if data.size()[0] < 2:\n",
    "                    continue;\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "                # clear the gradients\n",
    "                optimizer.zero_grad()\n",
    "                # make a forward pass\n",
    "                output = model(data)\n",
    "                # calculate the loss\n",
    "                loss = criterion(output, labels)\n",
    "                # do a backwards pass\n",
    "                loss.backward()\n",
    "                # perform a single optimization step\n",
    "                optimizer.step()\n",
    "                # update training loss\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "                # if self.sch_flag == True:\n",
    "                #  scheduler.step(train_loss)\n",
    "            # average losses\n",
    "            train_loss = train_loss / len(self.train_loader.dataset)\n",
    "            e_loss.append(train_loss)\n",
    "\n",
    "            # self.learning_rate = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        total_loss = sum(e_loss) / len(e_loss)\n",
    "\n",
    "        return model.state_dict(), total_loss\n",
    "\n",
    "\n",
    "\"\"\"### Server Side Training\n",
    "\n",
    "Following Algorithm 1 from the paper\n",
    "\"\"\"\n",
    "\n",
    "def training(model, rounds, batch_size, lr, ds, data_dict, C, K, E, plt_title, plt_color, cifar_data_test,\n",
    "             test_batch_size, criterion, num_classes, classes_test, sch_flag, filename):\n",
    "    \"\"\"\n",
    "    Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
    "    Specifically, this function is used for the server side training and weight update\n",
    "\n",
    "    Params:\n",
    "      - model:           PyTorch model to train\n",
    "      - rounds:          Number of communication rounds for the client update\n",
    "      - batch_size:      Batch size for client update training\n",
    "      - lr:              Learning rate used for client update training\n",
    "      - ds:              Dataset used for training\n",
    "      - data_dict:       Type of data partition used for training (IID or non-IID)\n",
    "      - C:               Fraction of clients randomly chosen to perform computation on each round\n",
    "      - K:               Total number of clients\n",
    "      - E:               Number of training passes each client makes over its local dataset per round\n",
    "      - tb_writer_name:  Directory name to save the tensorboard logs\n",
    "    Returns:\n",
    "      - model:           Trained model on the server\n",
    "    \"\"\"\n",
    "    \n",
    "    # global model weights\n",
    "    global_weights = model.state_dict()\n",
    "\n",
    "    # training loss\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    best_accuracy = 0\n",
    "    # measure time\n",
    "    start = time.time()\n",
    "\n",
    "    for curr_round in range(1, rounds + 1):\n",
    "        w, local_loss = [], []\n",
    "        # Retrieve the number of clients participating in the current training\n",
    "        m = max(int(C * K), 1)\n",
    "        # Sample a subset of K clients according with the value defined before\n",
    "        S_t = np.random.choice(range(K), m, replace=False)\n",
    "        # For the selected clients start a local training\n",
    "        for k in tqdm(S_t):\n",
    "            # Compute a local update\n",
    "            local_update = ClientUpdate(dataset=ds, batchSize=batch_size, learning_rate=lr, epochs=E, idxs=data_dict[k],\n",
    "                                        sch_flag=sch_flag)\n",
    "            # Update means retrieve the values of the network weights\n",
    "            weights, loss = local_update.train(model=copy.deepcopy(model))\n",
    "\n",
    "            w.append(copy.deepcopy(weights))\n",
    "            local_loss.append(copy.deepcopy(loss))\n",
    "        # lr = 0.999*lr\n",
    "        # updating the global weights\n",
    "        weights_avg = copy.deepcopy(w[0])\n",
    "        for k in weights_avg.keys():\n",
    "            for i in range(1, len(w)):\n",
    "                weights_avg[k] += w[i][k]\n",
    "\n",
    "            weights_avg[k] = torch.div(weights_avg[k], len(w))\n",
    "\n",
    "        global_weights = weights_avg\n",
    "\n",
    "        if curr_round == 200:\n",
    "            lr = lr / 2\n",
    "            E = E - 1\n",
    "\n",
    "        if curr_round == 300:\n",
    "            lr = lr / 2\n",
    "            E = E - 2\n",
    "\n",
    "        if curr_round == 400:\n",
    "            lr = lr / 5\n",
    "            E = E - 3\n",
    "\n",
    "        # move the updated weights to our model state dict\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # loss\n",
    "        loss_avg = sum(local_loss) / len(local_loss)\n",
    "        # print('Round: {}... \\tAverage Loss: {}'.format(curr_round, round(loss_avg, 3)), lr)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        t_accuracy, t_loss = testing(model, cifar_data_test, test_batch_size, criterion, num_classes, classes_test)\n",
    "        test_accuracy.append(t_accuracy)\n",
    "        test_loss.append(t_loss)\n",
    "\n",
    "        if best_accuracy < t_accuracy:\n",
    "            best_accuracy = t_accuracy\n",
    "        # torch.save(model.state_dict(), plt_title)\n",
    "        if filename is not None:\n",
    "            with open(filename, 'a') as f:\n",
    "                # create the csv writer\n",
    "                writer = csv.writer(f)\n",
    "\n",
    "                # write a row to the csv file\n",
    "                writer.writerow([curr_round, loss_avg, t_loss, t_accuracy, best_accuracy])\n",
    "        print(f\"Current Round: {curr_round}, Average Loss: {loss_avg}, Test Loss: {t_loss}, Test Accuracy: {t_accuracy}, Best: {best_accuracy}\")\n",
    "        # print('best_accuracy:', best_accuracy, '---Round:', curr_round, '---lr', lr, '----localEpocs--', E)\n",
    "\n",
    "    end = time.time()\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    fig, ax = plt.subplots()\n",
    "    x_axis = np.arange(1, rounds + 1)\n",
    "    y_axis1 = np.array(train_loss)\n",
    "    y_axis2 = np.array(test_accuracy)\n",
    "    y_axis3 = np.array(test_loss)\n",
    "\n",
    "    ax.plot(x_axis, y_axis1, 'tab:' + 'green', label='train_loss')\n",
    "    ax.plot(x_axis, y_axis2, 'tab:' + 'blue', label='test_accuracy')\n",
    "    ax.plot(x_axis, y_axis3, 'tab:' + 'red', label='test_loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set(xlabel='Number of Rounds', ylabel='Train Loss',\n",
    "           title=plt_title)\n",
    "    ax.grid()\n",
    "    # fig.savefig(plt_title+'.jpg', format='jpg')\n",
    "    print(\"Training Done!\")\n",
    "    print(f\"Best Accuracy: {best_accuracy}\")\n",
    "    print(\"Total time taken to Train: {}\".format(end - start))\n",
    "\n",
    "    return model\n",
    "\n",
    "\"\"\"## ResNet50 Model (W & W/O GN)\n",
    "\n",
    "> Indented block\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MyGroupNorm(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(MyGroupNorm, self).__init__()\n",
    "        self.norm = nn.GroupNorm(num_groups=2, num_channels=num_channels,\n",
    "                                 eps=1e-5, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"## Testing Loop\"\"\"\n",
    "\n",
    "\n",
    "def testing(model, dataset, bs, criterion, num_classes, classes):\n",
    "    # test loss\n",
    "    test_loss = 0.0\n",
    "    correct_class = list(0. for i in range(num_classes))\n",
    "    total_class = list(0. for i in range(num_classes))\n",
    "\n",
    "    test_loader = DataLoader(dataset, batch_size=bs)\n",
    "    l = len(test_loader)\n",
    "    model.eval()\n",
    "    for data, labels in test_loader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        test_loss += loss.item() * data.size(0)\n",
    "\n",
    "        _, pred = torch.max(output, 1)\n",
    "\n",
    "        correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(\n",
    "            correct_tensor.cpu().numpy())\n",
    "\n",
    "        # test accuracy for each object class\n",
    "        for i in range(data.size(0)):\n",
    "            label = labels.data[i]\n",
    "            correct_class[label] += correct[i].item()\n",
    "            total_class[label] += 1\n",
    "\n",
    "    # avg test loss\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    return 100. * np.sum(correct_class) / np.sum(total_class), test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
